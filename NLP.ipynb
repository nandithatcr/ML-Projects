{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Remove Stop Words**"
      ],
      "metadata": {
        "id": "vDZOUTsd6v1O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqnMO1dX6rjC",
        "outputId": "033a0c42-2eb1-4a50-d767-193855606993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'Sentence']\n",
            "['sample', 'Sentence']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "text = 'This is a sample Sentence'\n",
        "words = word_tokenize(text)\n",
        "print(words)\n",
        "filtered = [w for w in words if w.lower() not in stopwords.words('english')] #fetches the words that are not stopwords\n",
        "print(filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tokenization**"
      ],
      "metadata": {
        "id": "QyeKi4ynBpe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize('This is a sentence'))\n"
      ],
      "metadata": {
        "id": "3AruZWAc9tzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfb0ffa-9d36-4ca9-bb41-16e480df03bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sentence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Stemming**"
      ],
      "metadata": {
        "id": "9mfFrq7CCfuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "print(stemmer.stem('Running'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyCF_kawB2ls",
        "outputId": "be15502a-cbce-4502-dbd4-b072ac0974f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Lemmatization**"
      ],
      "metadata": {
        "id": "VeqVRnweCweM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize('running',pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inMWcanWCs8j",
        "outputId": "c56ff802-451a-42ca-e90c-e2a91984b2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'v' means verb\n",
        "\n",
        "'n' means noun (this is the default if you don’t specify pos)\n",
        "\n",
        "'a' means adjective\n"
      ],
      "metadata": {
        "id": "d1BwQeXGDWpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Lemmatization V/s Stemming**"
      ],
      "metadata": {
        "id": "UwzPuZcFDsui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word1 = 'running'\n",
        "word2 = 'better'\n",
        "word3 = 'studies'\n",
        "\n",
        "print('Stemming:')\n",
        "print(stemmer.stem(word1))\n",
        "print(stemmer.stem(word2))\n",
        "print(stemmer.stem(word3))\n",
        "\n",
        "print('\\nLemmatization')\n",
        "print(lemmatizer.lemmatize(word1,pos='v'))\n",
        "print(lemmatizer.lemmatize(word2,pos='a'))\n",
        "print(lemmatizer.lemmatize(word3,pos='n'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BTYjsiiDIFB",
        "outputId": "938ca9b6-3799-4df4-e720-5b5efdf9baad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming:\n",
            "run\n",
            "better\n",
            "studi\n",
            "\n",
            "Lemmatization\n",
            "run\n",
            "good\n",
            "study\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***POS***"
      ],
      "metadata": {
        "id": "VPAL6QSeEruf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#Downlaod the POS Tagger data\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt') #Also needed for word_tokenize\n",
        "\n",
        "from  nltk.tokenize import word_tokenize\n",
        "#Tokenize the sentence\n",
        "tokens =  word_tokenize('This is a sample sentence')\n",
        "\n",
        "#Get POS Tags\n",
        "print(nltk.pos_tag(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGpyAkv4En89",
        "outputId": "929968fd-f2e1-4e2d-a4bf-c2863f04620c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Named Entity Recogonition**"
      ],
      "metadata": {
        "id": "81h8VZz7Igw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp =  spacy.load('en_core_web_sm')\n",
        "doc = nlp('Barack Obama was born in Hawai')\n",
        "print([(ent.text,ent.label_)for ent in doc.ents])\n"
      ],
      "metadata": {
        "id": "vTbE5NxEFtIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ea0c8d-f1d9-4c64-a6e6-88e430f54414"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Barack Obama', 'PERSON'), ('Hawai', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy #this is library contains ready to use functions.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u4Qh5T7K9Od",
        "outputId": "5a39574f-0f90-41b1-eb9c-b2b3ef0125b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chunking**"
      ],
      "metadata": {
        "id": "42ljgximJMp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "text =  \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tags = nltk.pos_tag(tokens)\n",
        "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\" #This is for picking determinant,noun etc\n",
        "cp = nltk.RegexpParser(chunk_grammar)\n",
        "tree = cp.parse(tags)\n",
        "tree.pretty_print()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idw_SGx3JGap",
        "outputId": "8dbe5883-d9ed-42bd-8b1b-6e9d2d637882"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     S                                 \n",
            "     ________________________________|______________________            \n",
            "    |        |              NP               NP             NP         \n",
            "    |        |       _______|________        |       _______|______     \n",
            "jumps/VBZ over/IN The/DT quick/JJ brown/NN fox/NN the/DT lazy/JJ dog/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Word Emdedding or Vectorization**"
      ],
      "metadata": {
        "id": "7lPnoLYwNNtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(['This is a sample','This is another example'])\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtY0r_Q0Jyn9",
        "outputId": "4099a149-0424-453e-e0e3-1c2031a23479"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 1 1]\n",
            " [1 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Index:   0    1      2      3        4\n",
        "- Words: ['another', 'example', 'is', 'sample', 'this'] Here check another is present in first sentence. since its not there 0 similarly example is not there so 0. 'is' is present so 1 like that we are getting the output.  "
      ],
      "metadata": {
        "id": "1Q3lnQ1CNvWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv.get_feature_names_out()) #This gives the bag of words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQl6vkBbNk7I",
        "outputId": "45e0e2cf-90b7-48c9-e50f-781b5fb8dee4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['another' 'example' 'is' 'sample' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TF-IDF**"
      ],
      "metadata": {
        "id": "8Z13eDJdOIuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TF = Term Frequency → How often a word appears in a document.\n",
        "\n",
        "- IDF = Inverse Document Frequency → Measures how important a word is across all documents.\n",
        "\n",
        " - Words that appear in many documents get down-weighted.\n",
        "\n",
        "- TF-IDF = TF × IDF → Higher value means the word is important for that document but rare across others."
      ],
      "metadata": {
        "id": "F2nYZ4mjOzB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer()\n",
        "X = tf.fit_transform([\"This is a sample\", \"This is another example\"])\n",
        "print(tf.get_feature_names_out()) # this gives the bag of words\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8ZBAA12OFPx",
        "outputId": "a06bf5db-1878-4227-d387-4e4341e434f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['another' 'example' 'is' 'sample' 'this']\n",
            "[[0.         0.         0.50154891 0.70490949 0.50154891]\n",
            " [0.57615236 0.57615236 0.40993715 0.         0.40993715]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sample\n",
        "\n",
        "- another : 0\n",
        "- example: 0\n",
        "- is: 0.5 (TF-ID Score)\n",
        "- sample - 0.75 (TF-ID Score more unique so TFID score will be more)\n",
        "- this - 0.5\n",
        "\n",
        "This is another example\n",
        "- another : 0.5\n",
        "- example: 0.5 (This is similar to sample so TF-ID score will be less bcz it already learned\n",
        "- is: 0.5 (TF-ID Score)\n",
        "- sample - 0\n",
        "- this - 0.5 (Repeating word so score would be same)"
      ],
      "metadata": {
        "id": "xE2Bh1AcPWv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**n-grams range with Countvectorizer**"
      ],
      "metadata": {
        "id": "Yl2gHzLPTs_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "text = [\"I love NLP\"]\n",
        "cv = CountVectorizer(ngram_range=(1, 3)) #(1,3) determines how the model should study. it check simgle words bigrams and trigrams\n",
        "print(cv)\n",
        "X = cv.fit_transform(text)\n",
        "print(X)\n",
        "print(cv.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3pjx9H7PMTV",
        "outputId": "d841c94d-c01f-4c41-bfa4-8ba955f07494"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer(ngram_range=(1, 3))\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 3 stored elements and shape (1, 3)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 1)\t1\n",
            "['love' 'love nlp' 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- (1, 1) (default)\tOnly single words (unigrams)\n",
        "- (1, 2)\tUnigrams and bigrams\n",
        "- (2, 2)\tOnly bigrams\n",
        "- (1, 3)\tUnigrams, bigrams, and trigrams"
      ],
      "metadata": {
        "id": "AqxaRNhLTqe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sentiment Analysis**"
      ],
      "metadata": {
        "id": "8M-4GYxMUWIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "print(TextBlob(\"I love this product\").sentiment) #xtblob library is used for sentiment analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCtzKe5QSIvO",
        "outputId": "76506c3d-285f-4a59-ef3f-12a6628edc92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.5, subjectivity=0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Term             | Simple meaning                     |\n",
        "| ---------------- | ---------------------------------- |\n",
        "| **Polarity**     | How negative or positive it is     |\n",
        "| **Subjectivity** | How much of it is opinion vs. fact |\n",
        "\n",
        "\n",
        " <b>Polarity: 0.5 </b> <br> <hr>\n",
        "Range: from -1.0 (most negative) to +1.0 (most positive) <br>\n",
        "Meaning: A value of 0.5 indicates a moderately positive sentiment. <br>\n",
        "Negative: < 0 <br>\n",
        "Neutral: 0 <br>\n",
        "Positive: > 0 <br>\n",
        "\n",
        " <b>Subjectivity: 0.6 </b> <br> <hr>\n",
        "Range: from 0.0 (objective) to 1.0 (subjective)<br>\n",
        "Meaning: A value of 0.6 suggests the text is fairly subjective, meaning it's based more on personal opinion or feelings than on factual information.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vkb_mjYTU3bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Term             | Range                      | Meaning                                      |\n",
        "| ---------------- | -------------------------- | -------------------------------------------- |\n",
        "| **Polarity**     | `-1.0` to `+1.0`           | How **negative** or **positive** the text is |\n",
        "|                  | `-1.0` → Very negative     | Example: *“I hate this movie.”*              |\n",
        "|                  | `0.0` → Neutral            | Example: *“It is a movie.”*                  |\n",
        "|                  | `+1.0` → Very positive     | Example: *“I love this movie!”*              |\n",
        "|                  |                            |                                              |\n",
        "| **Subjectivity** | `0.0` to `1.0`             | How much is **opinion** vs. **fact**         |\n",
        "|                  | `0.0` → Completely factual | Example: *“The sun rises in the east.”*      |\n",
        "|                  | `1.0` → Fully opinionated  | Example: *“I think this is the best ever!”*  |\n"
      ],
      "metadata": {
        "id": "IXLpaOe9VCDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Text Classification with pipeline**"
      ],
      "metadata": {
        "id": "mdJAj6xcVcOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Example text data (spam detection style)\n",
        "texts = [\n",
        "    \"Buy now\",\n",
        "    \"Limited offer\",\n",
        "    \"Call me later\",\n",
        "    \"Meeting at noon\",\n",
        "    \"Free coupons\",\n",
        "    \"Let’s have lunch\",\n",
        "    \"Win a prize now\",\n",
        "    \"Project discussion tomorrow\"\n",
        "]\n",
        "labels = [\n",
        "    \"spam\",  # Buy now\n",
        "    \"spam\",  # Limited offer\n",
        "    \"ham\",   # Call me later\n",
        "    \"ham\",   # Meeting at noon\n",
        "    \"spam\",  # Free coupons\n",
        "    \"ham\",   # Let’s have lunch\n",
        "    \"spam\",  # Win a prize now\n",
        "    \"ham\"    # Project discussion tomorrow\n",
        "]\n",
        "\n",
        "#Split into training data and testing data\n",
        "x_train,x_test,y_train,y_test = train_test_split(\n",
        "    texts, labels, test_size=0.25,random_state=42\n",
        ")\n",
        "\n",
        "#Create the pipeline\n",
        "pipe = Pipeline([\n",
        "       ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "#Train the model\n",
        "pipe.fit(x_train,y_train)\n",
        "\n",
        "#Make the predictions\n",
        "preds = pipe.predict(x_test)\n",
        "\n",
        "#  Evaluate the model\n",
        "print(\"Test predictions:\", preds)\n",
        "print(\"Actual labels:   \", y_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
        "\n",
        "#  Try a new example\n",
        "new_text = [\"Congratulations, you won a free ticket!\"]\n",
        "new_pred = pipe.predict(new_text)\n",
        "print(\"\\nNew text:\", new_text[0])\n",
        "print(\"Predicted label:\", new_pred[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQJTzI3mUsGe",
        "outputId": "1d5a075a-4c31-4a7e-da2e-fb5495f56b89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test predictions: ['ham' 'ham']\n",
            "Actual labels:    ['spam', 'ham']\n",
            "Accuracy: 0.5\n",
            "\n",
            "New text: Congratulations, you won a free ticket!\n",
            "Predicted label: spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86g6Q6DNXmXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}